{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ckf9HBuk4bah"
   },
   "source": [
    "# Gym environment for a High Contrast Imaging instrument's AO control system.\n",
    "\n",
    "## Attributes:\n",
    "This is a simplified HCI system environment. The dynamics of the environment is based on a Markov Decision Process. The relevant attributes are the \"state\", \"action\", \"reward\".  \n",
    "The state in this case consists of 2 channels. The first is the WFS recorded phase projected on the DM actuator mode basis. For example, with a 25x25 actuator DM the state is a 25x25 matrix of actuator values. The second is the current DM actuator amplitudes. So overall, the state is a 25x25x2 tensor.   \n",
    "The action is the actuator amplitudes to be applied on the DM for the current timestep. In the case of a DM with 25x25 actuators, it is simply a 25x25 matrix.    \n",
    "The reward is a single scalar value. We can use the Strehl, the contrast, or any other such metric that needs to be maximized through AO control.  \n",
    "\n",
    "## Description:\n",
    "1. The optical system is defined in a class that inherits from OpenAI's gym environment class.\n",
    "2. This class contains 3 main methods - \\_\\_init\\_\\_(...), step(...), reset(...).\n",
    "3. The \\_\\_init\\_\\_(...) method sets up the optical system with the following components in order: a turbulence generator, a demagnifier, a DM, a WFS, a coronagraph, a quasi-static NCPA aberration, and a detector.\n",
    "4. The step(action) function does the following:\n",
    "   * The DM is updated with the actuator values specified in the \"action\".\n",
    "   * The wavefront is propagated through the science optical path.\n",
    "   * The instantaneous focal plane image is calculated.\n",
    "   * A reward is calculated using the metric function on the focal plane image, and is used to update the reward variable.\n",
    "   * The timestep is incremented by 1, and the turbulence generator is evolved accordingly.\n",
    "   * The WFS measurement is read out. The state variable for WFS measurement is updated.\n",
    "5. A reinforcement learning algorithm works in this environment to maximize expected future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJMsldF_5U70"
   },
   "source": [
    "### Step 0: ONLY RUN THIS ON COLAB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lX6JfIQ35Vjo"
   },
   "outputs": [],
   "source": [
    "!pip uninstall hcipy\n",
    "!rm -rf hcipy\n",
    "!git clone https://github.com/ehpor/hcipy.git\n",
    "!cd hcipy; git pull\n",
    "!cd hcipy; python setup.py install\n",
    "\n",
    "!mkdir agents\n",
    "!mkdir models\n",
    "!mkdir envs\n",
    "!mkdir utils\n",
    "!mkdir coronagraphs\n",
    "\n",
    "!mv agent.py agents/.\n",
    "!mv model.py models/.\n",
    "!mv HCI_TestBench.py envs/.\n",
    "!mv noise_model.py utils/.\n",
    "!mv replay_buffer.py utils/.\n",
    "!mv *.fits coronagraphs/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now restart the kernel before running the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_227Uh2v5wz8"
   },
   "source": [
    "### Step 1: Importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_-ee-0x5Z6_"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "from hcipy import *\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import os, glob\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GUTf4Eqo50Gj"
   },
   "outputs": [],
   "source": [
    "from envs.HCI_TestBench_residual import HCI_TestBench\n",
    "\n",
    "app_amp_file = \"coronagraphs/Square_20_80_20_25_0_2_amp_resampled_256.fits\"\n",
    "app_phase_file = \"coronagraphs/Square_20_80_20_25_0_2_phase_resampled_256.fits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddm8ZTXH6MwL"
   },
   "source": [
    "### Step 2: Define testbench parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cn7ScAI96Hlz"
   },
   "outputs": [],
   "source": [
    "# Create aperture and pupil/focal grids\n",
    "wavelength = 532e-9\n",
    "N = 256\n",
    "D = 10.5e-3\n",
    "pupil_grid = make_pupil_grid(N, D*1.1)\n",
    "science_focal_grid = make_focal_grid(8, 20, wavelength/D)\n",
    "aperture = circular_aperture(D)\n",
    "\n",
    "# Telescope parameters\n",
    "Dtel = 4\n",
    "tel_pupil_grid = make_pupil_grid(N, Dtel)\n",
    "tel_aperture = circular_aperture(Dtel)\n",
    "\n",
    "# Create the deformable mirror\n",
    "num_actuators = 25\n",
    "xinetics_basis = make_xinetics_influence_functions(pupil_grid, num_actuators, D * 1.1 / num_actuators)\n",
    "dm = DeformableMirror(xinetics_basis)\n",
    "num_modes = len(dm.influence_functions)\n",
    "dm.actuators = np.zeros(num_modes)\n",
    "\n",
    "## Create the Shack-Hartmann wavefront sensor and estimator\n",
    "## Create the microlens array\n",
    "F_mla = 30. / 0.3\n",
    "N_mla = 32\n",
    "D_mla = 10.5e-3\n",
    "\n",
    "shwfs = SquareShackHartmannWavefrontSensorOptics(pupil_grid, F_mla, N_mla, D_mla)\n",
    "shwfse = ShackHartmannWavefrontSensorEstimator(shwfs.mla_grid, shwfs.micro_lens_array.mla_index, circular_aperture(D)(shwfs.mla_grid).astype('bool'))\n",
    "\n",
    "# Atmosphere parameters\n",
    "velocity = 10 #m/s\n",
    "L0 = 40 # outer scale\n",
    "r0 = 0.2 # Fried parameter\n",
    "height = 0 # layer height\n",
    "timestep = 1e-3 # 1 ms per phasescreen\n",
    "\n",
    "# Make atmosphere\n",
    "layers = []\n",
    "layer = InfiniteAtmosphericLayer(tel_pupil_grid, Cn_squared_from_fried_parameter(r0, 500e-9), L0, velocity, height, stencil_length=2, use_interpolation=True)\n",
    "layers.append(layer)\n",
    "atmosphere = MultiLayerAtmosphere(layers, False)\n",
    "\n",
    "# Atmosphere parameters\n",
    "#pixels_per_frame = 1\n",
    "#velocity = np.array([pixels_per_frame,0])\n",
    "#L0 = 40\n",
    "#r0 = 0.2\n",
    "#height = 0\n",
    "\n",
    "# Make atmosphere\n",
    "#layers = []\n",
    "#layer = InfiniteAtmosphericLayer(tel_pupil_grid, Cn_squared_from_fried_parameter(r0, 500e-9), L0, velocity * tel_pupil_grid.delta[0], height, 2)\n",
    "#layers.append(layer)\n",
    "#atmosphere = MultiLayerAtmosphere(layers, False)\n",
    "\n",
    "## Create a demagnifier\n",
    "demag = Magnifier(D / Dtel)\n",
    "\n",
    "# Make initial phasescreen\n",
    "wf_tel = Wavefront(tel_aperture(tel_pupil_grid), wavelength)\n",
    "wf_tel.total_power = 100000\n",
    "wf = demag.forward(wf_tel)\n",
    "\n",
    "## Create propagator from pupil to focal plane\n",
    "prop = FraunhoferPropagator(pupil_grid, science_focal_grid)\n",
    "\n",
    "## Get the app coronagraph\n",
    "app_amp = fits.getdata(app_amp_file).ravel()\n",
    "app_phase = fits.getdata(app_phase_file).ravel()\n",
    "app = Apodizer(app_amp * np.exp(1j * app_phase))\n",
    "\n",
    "# For now we don't use a coronagraph so make it just a flat phase\n",
    "app = Apodizer(np.exp(1j * np.zeros(wf.phase.shape)))\n",
    "\n",
    "## Create detector\n",
    "science_camera = NoiselessDetector()\n",
    "\n",
    "## Generate a diffraction limited image for metrics\n",
    "diff_lim_img = prop(wf).power\n",
    "\n",
    "## Get the unit lambda/D\n",
    "l_D = wavelength / D\n",
    "plot_grid = make_focal_grid(8, 20, 1)\n",
    "\n",
    "## Create a noiseless camera image from the perfectly flat wavefront with coronograph\n",
    "wfdm = dm.forward(wf)\n",
    "wfapp = app.forward(wfdm)\n",
    "imapp = prop(wfapp).power\n",
    "dz_ind = np.where((imapp.grid.x >= (2 * l_D)) &\\\n",
    "                  (imapp.grid.x <= (8 * l_D)) &\\\n",
    "                  (imapp.grid.y >= (-3 * l_D)) &\\\n",
    "                  (imapp.grid.y <= (3 * l_D)))\n",
    "\n",
    "## Create an NCP aberration\n",
    "num_coeffs = 9\n",
    "plaw_index = -1\n",
    "np.random.seed(7)\n",
    "coeffs = ((np.random.rand(num_coeffs) - 0.5) * 2) * (np.arange(num_coeffs, dtype=float) + 1) ** plaw_index\n",
    "coeffs = np.zeros(coeffs.shape) # Set NCP to zero for now\n",
    "zernike_basis = make_zernike_basis(num_coeffs, D, pupil_grid, 2)\n",
    "ncp_phase = np.dot(zernike_basis.transformation_matrix, coeffs)\n",
    "ncp = Apodizer(np.exp(1j * ncp_phase))\n",
    "\n",
    "# Create an estimate of the NCP aberration for the forward model\n",
    "ncp_field_est = np.exp(1j * np.zeros(app_phase.shape))\n",
    "estimated_coeffs = np.zeros(coeffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "OajqgOZH7aeq",
    "outputId": "f72e7c4e-5caf-43cc-feae-7d3c7a9f188a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-calibrated control matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/miniconda3/envs/ai_esports/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "tb = HCI_TestBench(wf_tel, timestep, atmosphere, demag, dm, shwfs, shwfse, ncp, app, prop, science_camera, dz_ind, \"strehl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: See a standard linear controller in action\n",
    "In this section we will simply add the wavefront sensor measurement with the current DM actuator values. This should give a reasonably high Strehl / contrast. This is just to see that the testbench works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tb.reset() # Start with a new phasescreen\n",
    "\n",
    "for time in range(50):\n",
    "    \n",
    "    amps = state.copy()\n",
    "    amps = np.squeeze(amps)\n",
    "    amps = np.clip(amps, -1, 1)\n",
    "    \n",
    "    next_state, reward, done, _ = tb.step(amps)\n",
    "    \n",
    "    state = next_state.copy()\n",
    "    \n",
    "    print(\"\\rTime step: {} Strehl ratio: {:.3f}\".format(time, reward), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tb.reset() # Start with a new phasescreen\n",
    "\n",
    "for time in range(50):\n",
    "    \n",
    "    amps = state.copy()\n",
    "    amps = np.squeeze(amps)\n",
    "    amps = np.clip(amps, -1, 1)\n",
    "    amps = np.random.uniform(-0.02, 0.02, amps.shape)\n",
    "    \n",
    "    next_state, reward, done, _ = tb.step(amps)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    state = next_state.copy()\n",
    "    \n",
    "    print(\"\\rTime step: {} Strehl ratio: {:.3f}\".format(time, reward), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pltfig in range(100):\n",
    "    amps = state.copy()\n",
    "    amps = np.squeeze(amps)\n",
    "    amps = np.clip(amps, -1, 1)\n",
    "    \n",
    "    next_state, reward, done, _ = tb.step(amps)\n",
    "    state = next_state.copy()\n",
    "    \n",
    "    wfatms = tb._turbulence()\n",
    "    imshow_field(wfatms.phase, grid=pupil_grid, vmin=-np.pi, vmax=np.pi, cmap='RdBu')\n",
    "    plt.savefig(\"{:0>2d}.png\".format(pltfig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = state.copy()\n",
    "amps = np.squeeze(amps)\n",
    "amps = np.clip(amps, -1, 1)\n",
    "    \n",
    "next_state, reward, done, _ = tb.step(amps)\n",
    "state = next_state.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfatms = tb._turbulence()\n",
    "imshow_field(wfatms.phase, grid=pupil_grid, vmin=-np.pi, vmax=np.pi, cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tb._img\n",
    "imshow_field(np.log10(img / img.max()), vmin=-6, grid=plot_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with a simple addition operation the resulting Strehl is quite good, between 78 to 80 percent. We should be able to reproduce this with a well trained agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQea5RJ0LItE"
   },
   "source": [
    "### Step 4: Train the DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Umt8ZL_LSo5"
   },
   "outputs": [],
   "source": [
    "from agents.agent import DDPG\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_T10NGl_LlVT"
   },
   "outputs": [],
   "source": [
    "# Define all hyperparameters here\n",
    "ACTOR_LR = 1e-5\n",
    "CRITIC_LR = 1e-3\n",
    "RANDOM_SEED = 42\n",
    "MU = 0.0\n",
    "THETA = 0.15\n",
    "SIGMA = 1.0\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "TAU = 1e-2\n",
    "N_TIME_STEPS = 1\n",
    "N_LEARN_UPDATES = 1\n",
    "EPSILON_START = 0.01\n",
    "EPSILON_DECAY = 1\n",
    "EPSILON_END = 0.01\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    DEVICE = \"/GPU:0\"\n",
    "else:\n",
    "    DEVICE = \"/device:CPU:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHrX0V1iM3VK"
   },
   "outputs": [],
   "source": [
    "state_size = (25, 25, 1)\n",
    "action_size = (25, 25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDaqZ_LJMgRx"
   },
   "outputs": [],
   "source": [
    "agent = DDPG(state_size, action_size, ACTOR_LR, CRITIC_LR,\n",
    "             RANDOM_SEED, MU, THETA, SIGMA, BUFFER_SIZE, BATCH_SIZE,\n",
    "             EPSILON_START, EPSILON_END, EPSILON_DECAY,\n",
    "             GAMMA, TAU, N_TIME_STEPS, N_LEARN_UPDATES, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yprFnutRMyny",
    "outputId": "c1967fa7-c0bd-4a4d-bd6e-a06729466f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Time in ms:77 Score: 0.022"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = tb.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        t = 0\n",
    "        \n",
    "        while(t <= 300):\n",
    "            t += 1\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            next_state, reward, done, _ = tb.step(np.squeeze(action))\n",
    "            \n",
    "            if t == 300:\n",
    "                done = True\n",
    "            \n",
    "            agent.step(t, state, action, np.log10(reward), next_state, done)\n",
    "            \n",
    "            state = next_state.copy()\n",
    "            score += reward\n",
    "            \n",
    "            print('\\rEpisode {} Time in ms:{} Score: {:.3f}'.format(i_episode, t, reward), end=\"\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        scores_deque.append(score / t)\n",
    "        scores.append(score / t)\n",
    "        agent.actor_local.model.save('checkpoint_actor.h5')\n",
    "        agent.critic_local.model.save('checkpoint_critic.h5')\n",
    "        print('\\rEpisode {} Time in ms:{} Average Strehl: {:.3f}'.format(i_episode, t, (score / t)))\n",
    "        \n",
    "        if np.mean(scores_deque) >= 0.6:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            agent.actor_local.model.save('checkpoint_actor.h5')\n",
    "            agent.critic_local.model.save('checkpoint_critic.h5')\n",
    "            break\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a trained agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = tb.reset()\n",
    "    score = 0\n",
    "    t = 0\n",
    "        \n",
    "    while(t < 10):\n",
    "        t += 1\n",
    "        state = np.expand_dims(state, 0)\n",
    "        action = agent.actor_local.model(state)\n",
    "        action = action.numpy()[0]\n",
    "        next_state, reward, done, _ = tb.step(action)\n",
    "        print('\\rTime in ms:{}\\t Strehl: {:.2f}'.format(t, reward), end=\"\")\n",
    "        state = next_state\n",
    "        score += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tb.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.expand_dims(state, 0)\n",
    "action = agent.actor_local.model(state)\n",
    "action = action.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, _ = tb.step(action)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(state))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(action))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(state) - np.squeeze(action))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_field(np.log10(tb._img / tb._img.max()), vmin=-5, grid=plot_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Apodizer(app_amp * np.exp(1j * app_phase))\n",
    "\n",
    "tb_contrast = HCI_TestBench(wf_tel, timestep, atmosphere, demag, dm, shwfs, shwfse, ncp, app, prop, science_camera, dz_ind, rwrd_func=\"contrast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_START = 1e-3\n",
    "EPSILON_END = 1e-4\n",
    "\n",
    "agent_contrast = DDPG(state_size, action_size, ACTOR_LR, CRITIC_LR,\n",
    "                      RANDOM_SEED, MU, THETA, SIGMA, BUFFER_SIZE, BATCH_SIZE,\n",
    "                      EPSILON_START, EPSILON_END, EPSILON_DECAY,\n",
    "                      GAMMA, TAU, N_TIME_STEPS, N_LEARN_UPDATES, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_contrast.actor_local.model.set_weights(agent.actor_local.model.get_weights())\n",
    "agent_contrast.actor_target.model.set_weights(agent.actor_local.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_ctrl(n_episodes=100, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = tb_contrast.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        t = 0\n",
    "        \n",
    "        while(t < 10):\n",
    "            t += 1\n",
    "            action = agent_contrast.act(state)\n",
    "            next_state, reward, done, _ = tb_contrast.step(np.squeeze(action))\n",
    "            \n",
    "            if t == 9:\n",
    "                done = True\n",
    "            \n",
    "            agent_contrast.step(t, state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            print('\\rEpisode {} Time in ms:{}\\t Contrast: {:.2f}'.format(i_episode, t, reward), end=\"\")\n",
    "            \n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(score / t)\n",
    "        scores.append(score / t)\n",
    "        agent_contrast.actor_local.model.save('ctrst_actor.h5')\n",
    "        agent_contrast.critic_local.model.save('ctrst_critic.h5')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage contrast over the past 100 episodes: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) >= 7:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            agent_contrast.actor_local.model.save('checkpoint_actor.h5')\n",
    "            agent_contrast.critic_local.model.save('checkpoint_critic.h5')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = contrast_ctrl()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('log10 Contrast')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = tb_contrast.reset()\n",
    "state = np.expand_dims(state, 0)\n",
    "action = agent_contrast.actor_local.model(state)\n",
    "action = action.numpy()[0]\n",
    "#action = state[:, :, 0].copy() + state[:, :, 1].copy()\n",
    "#action = np.clip(action, -1, 1)\n",
    "next_state, reward, done, _ = tb_contrast.step(action.squeeze())\n",
    "\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_field(np.log10(tb_contrast._img / tb_contrast._img.max()), vmin=-5, grid=plot_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HCI_DRL_2Dstate.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
